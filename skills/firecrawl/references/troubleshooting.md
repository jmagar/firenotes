# Firecrawl Troubleshooting Guide

Common issues and solutions for Firecrawl CLI and API.

## Authentication Errors

### 401 Unauthorized

**Error:**
```
Error: 401 Unauthorized - Invalid or missing API key
```

**Causes:**
- Invalid API key
- API key not set in environment
- Expired API key
- Wrong API endpoint

**Solutions:**

1. Check API key in `.env` file:
```bash
cat ~/claude-homelab/.env | grep FIRECRAWL_API_KEY
```

2. Verify API key is correct:
```bash
# Get new key from https://firecrawl.dev/
# Update .env file
echo 'FIRECRAWL_API_KEY="fc-your-new-key"' >> ~/claude-homelab/.env
```

3. Test authentication:
```bash
firecrawl --status
```

4. Re-login if needed:
```bash
firecrawl logout
firecrawl login --api-key fc-YOUR-KEY
```

---

### API Key Required

**Error:**
```
Error: API key required for cloud API
```

**Causes:**
- `FIRECRAWL_API_KEY` not set
- Using cloud API without credentials

**Solutions:**

1. Add API key to `.env`:
```bash
echo 'FIRECRAWL_API_KEY="fc-your-api-key"' >> ~/claude-homelab/.env
```

2. Or provide via flag:
```bash
firecrawl https://example.com --api-key fc-YOUR-KEY
```

3. Or use self-hosted instance (no key required):
```bash
echo 'FIRECRAWL_API_URL="http://localhost:3002"' >> ~/claude-homelab/.env
echo 'FIRECRAWL_API_KEY=""' >> ~/claude-homelab/.env
```

---

## Connection Errors

### Connection Refused

**Error:**
```
Error: connect ECONNREFUSED 127.0.0.1:3002
```

**Causes:**
- Self-hosted Firecrawl not running
- Wrong API URL
- Network issues

**Solutions:**

1. Check if service is running (self-hosted):
```bash
docker ps | grep firecrawl
# Or
curl http://localhost:3002/health
```

2. Verify API URL in `.env`:
```bash
cat ~/claude-homelab/.env | grep FIRECRAWL_API_URL
```

3. Start service if stopped:
```bash
docker compose up -d firecrawl
```

4. Check firewall rules:
```bash
sudo ufw status
```

---

### Timeout Errors

**Error:**
```
Error: Request timeout after 30000ms
```

**Causes:**
- Website not responding
- Slow JavaScript rendering
- Network latency
- Rate limiting

**Solutions:**

1. Increase wait time for JavaScript sites:
```bash
firecrawl https://example.com --wait-for 10000
```

2. Check website manually:
```bash
curl -I https://example.com
```

3. Try with reduced concurrency:
```bash
firecrawl crawl https://example.com --max-concurrency 1 --wait
```

4. Add delay between requests:
```bash
firecrawl crawl https://example.com --delay 2000 --wait
```

---

## Rate Limiting

### 429 Too Many Requests

**Error:**
```
Error: 429 Too Many Requests - Rate limit exceeded
```

**Causes:**
- Too many requests in short time
- Concurrency limit reached
- Server protection triggered

**Solutions:**

1. Add delay between requests:
```bash
firecrawl crawl https://example.com --delay 1000 --wait
```

2. Reduce concurrency:
```bash
firecrawl crawl https://example.com --max-concurrency 2 --wait
```

3. Check rate limit status:
```bash
firecrawl --status
```

4. Wait before retrying:
```bash
sleep 60
firecrawl https://example.com
```

---

### Concurrency Limit Reached

**Error:**
```
Error: Concurrency limit reached (100/100)
```

**Causes:**
- Too many jobs running simultaneously
- Previous jobs not completed

**Solutions:**

1. Check current status:
```bash
firecrawl --status
```

2. Wait for existing jobs to complete
3. Use async mode and poll later:
```bash
# Start crawl (returns job ID)
firecrawl crawl https://example.com --limit 50

# Check status later
firecrawl --status
```

4. Cancel running jobs if needed (via web UI)

---

## Data Extraction Issues

### No Content Extracted

**Error:**
```
Warning: No content extracted from page
```

**Causes:**
- JavaScript-rendered content not loaded
- Page requires authentication
- Content behind paywall
- Anti-scraping protection

**Solutions:**

1. Wait for JavaScript to render:
```bash
firecrawl https://example.com --wait-for 5000
```

2. Check if content is main article:
```bash
firecrawl https://example.com --only-main-content
```

3. Try different format:
```bash
firecrawl https://example.com --format html
```

4. Verify manually in browser:
```bash
curl https://example.com | grep "expected content"
```

---

### Too Much Clutter

**Error:**
```
Output includes navigation, ads, footers
```

**Causes:**
- Not using main content filter
- Page structure unclear
- Multiple content sections

**Solutions:**

1. Use main content filter:
```bash
firecrawl https://example.com --only-main-content
```

2. Exclude specific tags:
```bash
firecrawl https://example.com --exclude-tags "nav,footer,aside,script,style"
```

3. Include only content tags:
```bash
firecrawl https://example.com --include-tags "article,main,section,p,h1,h2,h3"
```

4. Use markdown format for cleaner output:
```bash
firecrawl https://example.com --format markdown --only-main-content
```

---

### Missing Specific Elements

**Error:**
```
Expected elements not in output
```

**Causes:**
- Tag filtering too strict
- Elements loaded by JavaScript
- Elements in excluded sections

**Solutions:**

1. Check tag filters:
```bash
# Remove exclude-tags if used
firecrawl https://example.com
```

2. Wait for dynamic content:
```bash
firecrawl https://example.com --wait-for 5000
```

3. Use HTML format to preserve structure:
```bash
firecrawl https://example.com --format html
```

4. Try raw HTML:
```bash
firecrawl https://example.com --format rawHtml
```

---

## Crawl Issues

### Crawl Stopped Early

**Error:**
```
Crawl completed with fewer pages than expected
```

**Causes:**
- Hit page limit
- Reached max depth
- No more links to follow
- Path filtering too restrictive

**Solutions:**

1. Increase page limit:
```bash
firecrawl crawl https://example.com --limit 500 --wait
```

2. Increase max depth:
```bash
firecrawl crawl https://example.com --max-depth 5 --wait
```

3. Check path filters:
```bash
# Remove or adjust include-paths
firecrawl crawl https://example.com --wait
```

4. Enable entire domain:
```bash
firecrawl crawl https://example.com --crawl-entire-domain --wait
```

---

### Crawl Too Slow

**Error:**
```
Crawl taking too long to complete
```

**Causes:**
- Rate limiting active
- Low concurrency
- Large delay between requests

**Solutions:**

1. Reduce delay:
```bash
firecrawl crawl https://example.com --delay 500 --wait
```

2. Increase concurrency:
```bash
firecrawl crawl https://example.com --max-concurrency 10 --wait
```

3. Use async mode (non-blocking):
```bash
# Start crawl (returns job ID)
firecrawl crawl https://example.com --limit 100

# Check progress later
firecrawl --status
```

4. Limit scope:
```bash
firecrawl crawl https://example.com --limit 100 --max-depth 2 --wait
```

---

### Paths Not Included

**Error:**
```
Expected paths missing from crawl
```

**Causes:**
- Path filtering incorrect
- Pattern syntax wrong
- Paths not linked from start URL

**Solutions:**

1. Check include-paths pattern:
```bash
# Use wildcards correctly
firecrawl crawl https://example.com --include-paths "/blog/*" --wait
```

2. Use multiple patterns:
```bash
firecrawl crawl https://example.com --include-paths "/blog/*,/docs/*" --wait
```

3. Map site first to see structure:
```bash
firecrawl map https://example.com --search "blog"
```

4. Remove path filters:
```bash
firecrawl crawl https://example.com --wait
```

---

## Output Issues

### Invalid JSON

**Error:**
```
SyntaxError: Unexpected token in JSON
```

**Causes:**
- Mixing single and multiple formats
- Piping errors with output
- Incomplete response

**Solutions:**

1. Use pretty-print for JSON:
```bash
firecrawl https://example.com --format markdown,html --pretty
```

2. Save to file instead of stdout:
```bash
firecrawl https://example.com --format markdown,html -o output.json
```

3. Redirect errors separately:
```bash
firecrawl https://example.com 2> errors.log
```

4. Validate JSON with jq:
```bash
firecrawl https://example.com --format markdown,html | jq .
```

---

### File Save Failed

**Error:**
```
Error: ENOENT: no such file or directory
```

**Causes:**
- Output directory doesn't exist
- No write permissions
- Invalid file path

**Solutions:**

1. Create output directory:
```bash
mkdir -p output/
firecrawl https://example.com -o output/page.md
```

2. Check permissions:
```bash
ls -la output/
chmod 755 output/
```

3. Use absolute path:
```bash
firecrawl https://example.com -o /home/user/output/page.md
```

4. Test with temp directory:
```bash
firecrawl https://example.com -o /tmp/test.md
```

---

## Credit and Quota Issues

### Out of Credits

**Error:**
```
Error: Insufficient credits
```

**Causes:**
- Account credits depleted
- Free tier limit reached

**Solutions:**

1. Check credit usage:
```bash
firecrawl credit-usage
```

2. Check account status:
```bash
firecrawl --status
```

3. Upgrade plan or add credits (via web UI)

4. Use self-hosted instance:
```bash
echo 'FIRECRAWL_API_URL="http://localhost:3002"' >> ~/claude-homelab/.env
```

---

### Quota Exceeded

**Error:**
```
Error: Monthly quota exceeded
```

**Causes:**
- Used all monthly quota
- Plan limit reached

**Solutions:**

1. Wait for quota reset:
```bash
firecrawl credit-usage
# Check resetDate
```

2. Upgrade plan (via web UI)

3. Use self-hosted for unlimited quota

4. Optimize requests:
```bash
# Use main content only
firecrawl https://example.com --only-main-content

# Reduce crawl scope
firecrawl crawl https://example.com --limit 50 --max-depth 2 --wait
```

---

## Environment Configuration

### .env File Not Found

**Error:**
```
ERROR: .env file not found at ~/claude-homelab/.env
```

**Causes:**
- `.env` file doesn't exist
- Wrong file location
- Incorrect permissions

**Solutions:**

1. Create `.env` file:
```bash
touch ~/claude-homelab/.env
chmod 600 ~/claude-homelab/.env
```

2. Add credentials:
```bash
cat >> ~/claude-homelab/.env <<EOF
FIRECRAWL_API_KEY="fc-your-api-key"
FIRECRAWL_API_URL="https://api.firecrawl.dev"
EOF
```

3. Verify file exists:
```bash
ls -la ~/claude-homelab/.env
```

4. Check file permissions:
```bash
chmod 600 ~/claude-homelab/.env
```

---

### Environment Variables Not Loaded

**Error:**
```
Using credentials from .env but getting auth errors
```

**Causes:**
- Variables not exported
- Wrong variable names
- Syntax errors in `.env`

**Solutions:**

1. Check `.env` syntax:
```bash
cat ~/claude-homelab/.env
# Look for syntax errors (quotes, spacing)
```

2. Source manually to test:
```bash
set -a
source ~/claude-homelab/.env
set +a
echo $FIRECRAWL_API_KEY
```

3. Verify variable names:
```bash
grep FIRECRAWL ~/claude-homelab/.env
# Should be FIRECRAWL_API_KEY and FIRECRAWL_API_URL
```

4. Use direct flag as workaround:
```bash
firecrawl https://example.com --api-key fc-YOUR-KEY
```

---

## Performance Issues

### Memory Errors

**Error:**
```
JavaScript heap out of memory
```

**Causes:**
- Large crawl results
- Too many concurrent requests
- Memory leak

**Solutions:**

1. Limit crawl scope:
```bash
firecrawl crawl https://example.com --limit 100 --max-depth 2 --wait
```

2. Use async mode:
```bash
# Non-blocking, process results incrementally
firecrawl crawl https://example.com --limit 500
```

3. Increase Node.js memory:
```bash
NODE_OPTIONS="--max-old-space-size=4096" firecrawl crawl https://example.com --wait
```

4. Process in batches:
```bash
# Map first to get URLs
firecrawl map https://example.com --limit 1000 -o urls.json

# Scrape URLs in batches
for url in $(jq -r '.urls[]' urls.json | head -100); do
    firecrawl "$url" >> results.md
done
```

---

### Disk Space Full

**Error:**
```
Error: ENOSPC: no space left on device
```

**Causes:**
- Large output files
- Multiple crawls saved
- Disk full

**Solutions:**

1. Check disk space:
```bash
df -h
```

2. Clean up old files:
```bash
rm -rf ~/claude-homelab/skills/firecrawl/output/*
```

3. Use streaming output:
```bash
firecrawl https://example.com | gzip > output.md.gz
```

4. Save to different mount:
```bash
firecrawl https://example.com -o /mnt/storage/output.md
```

---

## Common Command Issues

### Command Not Found

**Error:**
```
bash: npx: command not found
```

**Causes:**
- Node.js not installed
- npx not in PATH

**Solutions:**

1. Install Node.js:
```bash
# Ubuntu/Debian
sudo apt install nodejs npm

# Or use nvm
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
nvm install 18
```

2. Verify installation:
```bash
node --version
npm --version
npx --version
```

3. Install CLI globally:
```bash
npm install -g @firecrawl/cli
firecrawl --version
```

---

### Permission Denied

**Error:**
```
bash: ./scripts/scrape.sh: Permission denied
```

**Causes:**
- Script not executable
- Wrong file permissions

**Solutions:**

1. Make script executable:
```bash
chmod +x scripts/scrape.sh
```

2. Make all scripts executable:
```bash
chmod +x scripts/*.sh
```

3. Run with bash explicitly:
```bash
bash scripts/scrape.sh https://example.com
```

---

## Getting Help

### Check Logs

```bash
# Enable debug output
DEBUG=* firecrawl https://example.com

# Save output and errors
firecrawl https://example.com > output.log 2>&1
```

### Test Configuration

```bash
# Check status
firecrawl --status

# Test authentication
firecrawl login --api-key fc-YOUR-KEY

# View configuration
firecrawl config
```

### Contact Support

If issues persist:

1. Check [official documentation](https://docs.firecrawl.dev/)
2. Visit [GitHub issues](https://github.com/firecrawl/cli/issues)
3. Join [Discord community](https://discord.gg/firecrawl)
4. Contact support (cloud users)

---

## Additional Resources

- [Official Documentation](https://docs.firecrawl.dev/)
- [CLI Documentation](https://docs.firecrawl.dev/sdks/cli)
- [GitHub Repository](https://github.com/firecrawl/firecrawl)
- [CLI GitHub](https://github.com/firecrawl/cli)
- [API Endpoints Reference](./api-endpoints.md)
- [Quick Reference Guide](./quick-reference.md)
