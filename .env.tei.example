# TEI standalone environment example
# Copy to .env.tei and set values for your host/GPU
#
# After starting TEI, set TEI_URL in your main .env to point at this instance:
#   TEI_URL=http://<host>:<TEI_HTTP_PORT>

# Service endpoint (host high-port -> container 80)
TEI_HTTP_PORT=53020

# Model + runtime
TEI_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
TEI_DTYPE=float16
TEI_POOLING=last-token
TEI_DEFAULT_PROMPT=Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery:

# Throughput tuning for RTX 4070 (12GB VRAM)
TEI_MAX_CONCURRENT_REQUESTS=80
TEI_MAX_BATCH_TOKENS=163840
TEI_MAX_BATCH_REQUESTS=80
TEI_MAX_CLIENT_BATCH_SIZE=128
TEI_TOKENIZATION_WORKERS=8

# Storage
TEI_DATA_DIR=./data/tei

# GPU selection
NVIDIA_VISIBLE_DEVICES=0
CUDA_VISIBLE_DEVICES=0

# Runtime env
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8
TOKENIZERS_PARALLELISM=true
CUDA_CACHE_DISABLE=0
RUST_LOG=text_embeddings_router=info
HF_HUB_ENABLE_HF_TRANSFER=1

# Hugging Face token (required for gated/private models)
HF_TOKEN=
